# -*- coding: utf-8 -*-
"""DISS- CA 2 Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uRpvsU3ZLSm3WWzluchvZzYidXL7Z7C0
"""

!pip install pymongo[srv] pyspark==3.1.2 azure-storage-blob dnspython

"""Retrieving data from Azure Blob storage"""

from azure.storage.blob import BlobServiceClient
from io import StringIO
import pandas as pd

# Azure blob details
account_name = "dissproject1"
account_key = "account key" # Insert account key (Removed for protection)
container_name = "drug-reviews"
blob_name = "train.csv"

# Connecting and download
connect_str = f"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net"
blob_service_client = BlobServiceClient.from_connection_string(connect_str)
blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
csv_data = blob_client.download_blob().readall().decode('utf-8')

# Loading into pandas
df_csv = pd.read_csv(StringIO(csv_data))
df_csv = df_csv.dropna(subset=["review", "rating", "condition", "date"])
df_csv = df_csv.astype(str)  # Important for MongoDB compatibility

"""Storing the data in a MongoDB database"""

from pymongo import MongoClient

# MongoDB Atlas connection string
mongo_uri = "mongodb+srv://<username>:<password>@<cluster>.mongodb.net/?retryWrites=true&w=majority"
client = MongoClient(mongo_uri)

# Defining DB and collection
db = client["drug_review_db"]
collection = db["reviews"]

# Upload records
records = df_csv.to_dict("records")
collection.insert_many(records)

from pymongo import MongoClient
import pandas as pd

# MongoDB Atlas URI
mongo_uri = "mongodb+srv://alex#####:H8GiodDk5RzvT7E0@cluster0.t0e2n.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0" # Give the correct mongo Atlas URI

# Connecting and reading
client = MongoClient(mongo_uri)
collection = client["drug_review_db"]["reviews"]

# Loading into Pandas
mongo_data = pd.DataFrame(list(collection.find()))
mongo_data = mongo_data.drop(columns=["_id"])  # Droping MongoDB's internal _id

#Saving the Pandas DataFrame to a CSV file
mongo_data.to_csv("/tmp/mongo_data.csv", index=False)

#Reading the CSV file with Spark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("CSVtoSpark").getOrCreate()
df = spark.read.option("header", "true").option("inferSchema", "true").csv("/tmp/mongo_data.csv")

#Checking the schema
df.printSchema()
df.show(5, truncate=False)

"""Pre-Processing"""

from pyspark.sql.functions import col, regexp_replace, lower, trim, to_date

#Droping rows with nulls in essential columns
df_clean = df.dropna(subset=["drugName", "condition", "review", "rating", "date"])

#Removing duplicates
df_clean = df_clean.dropDuplicates()

#Fixing data types
df_clean = df_clean.withColumn("rating", col("rating").cast("float"))
df_clean = df_clean.withColumn("usefulCount", col("usefulCount").cast("int"))
df_clean = df_clean.withColumn("date", to_date(col("date"), "yyyy-MM-dd"))

#Basic text cleaning
df_clean = df_clean.withColumn("review", lower(col("review")))
df_clean = df_clean.withColumn("review", regexp_replace(col("review"), "[^a-zA-Z0-9\s]", ""))
df_clean = df_clean.withColumn("review", trim(col("review")))

#Check
df_clean.printSchema()
df_clean.show(5, truncate=False)

"""Analysis - Research questions"""

reviews_pd = df_clean.select("review").toPandas()

text = " ".join(reviews_pd["review"].dropna().tolist())

#Generating word cloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud of Drug Reviews", fontsize=18)
plt.show()

"""1.What are the emerging concerns and changes in public opinion (Trend analysis)?"""

from pyspark.sql.functions import year

# Checking where year is null
df_clean.withColumn("year", year("date")) \
    .filter("year IS NULL") \
    .select("date", "rating", "review") \
    .show(10, truncate=False)

from pyspark.sql.functions import col, to_date

# Converting to Spark date type
df_clean = df_clean.withColumn("date", to_date(col("date"), "yyyy-MM-dd"))

# Filtering out null date rows
df_clean_valid = df_clean.filter(col("date").isNotNull())

from pyspark.sql.functions import year, month, count, avg

# Trend over years - average rating and volume of reviews
trend_df = df_clean_valid.withColumn("year", year("date")) \
    .groupBy("year") \
    .agg(
        avg("rating").alias("average_rating"),
        count("*").alias("review_count")
    ) \
    .orderBy("year")

trend_df.show()

from pyspark.sql.functions import explode, split

# Extracting keywords from review
words_df = df_clean_valid.withColumn("year", year("date")) \
    .withColumn("word", explode(split("review", " "))) \
    .filter("length(word) > 4")  # filter short/stop words

# Counting word frequency per year
keyword_trend = words_df.groupBy("year", "word") \
    .count() \
    .orderBy("year", "count", ascending=[True, False])

keyword_trend.show(30, truncate=False)

from pyspark.ml.feature import Tokenizer, StopWordsRemover

# Tokenize reviews
tokenizer = Tokenizer(inputCol="review", outputCol="words_raw")
df_tokens = tokenizer.transform(df_clean_valid)

# Remove stopwords
remover = StopWordsRemover(inputCol="words_raw", outputCol="words_clean")
df_clean = remover.transform(df_tokens)

from pyspark.ml.feature import NGram

ngram = NGram(n=2, inputCol="words_clean", outputCol="bigrams")
df_bigrams = ngram.transform(df_clean)

# Explode bigrams for counting
from pyspark.sql.functions import explode

df_exploded = df_bigrams.select(explode("bigrams").alias("bigram"))

# Count bigram frequency
bigram_counts = df_exploded.groupBy("bigram").agg(count("*").alias("count"))
top_bigrams = bigram_counts.orderBy(col("count").desc()).limit(20)

top_bigrams.show(truncate=False)

import matplotlib.pyplot as plt

# Convert to pandas for plotting
top_bigrams_pd = top_bigrams.toPandas()

# Plotting
plt.figure(figsize=(12, 6))
plt.barh(top_bigrams_pd['bigram'], top_bigrams_pd['count'], color='skyblue')
plt.xlabel("Frequency")
plt.title("Top 20 Most Frequent Bigrams in Drug Reviews")
plt.gca().invert_yaxis()  # highest first
plt.tight_layout()
plt.show()



import matplotlib.pyplot as plt
import seaborn as sns

# For clean plots
sns.set(style="whitegrid")

trend_pd = trend_df.toPandas()

fig, ax1 = plt.subplots(figsize=(10,6))

# Line for avg rating
sns.lineplot(data=trend_pd, x="year", y="average_rating", marker='o', ax=ax1, label='Avg Rating', color='blue')
ax1.set_ylabel('Average Rating', color='blue')
ax1.set_xlabel('Year')

# Bar for review count
ax2 = ax1.twinx()
sns.barplot(data=trend_pd, x="year", y="review_count", alpha=0.4, ax=ax2, color='gray')
ax2.set_ylabel('Review Count', color='gray')

plt.title('Average Drug Ratings & Review Volume Over Time')
plt.show()



"""
2.How can clinicians be assisted to understand general perceptions of the patients regarding the drugs (Sentiment analysis)?"""

from pyspark.sql.functions import when

# Creating sentiment label
df_sentiment = df_clean.withColumn("sentiment",
    when(col("rating") >= 8, "positive")
    .when(col("rating") <= 4, "negative")
    .otherwise("neutral")
)

# Sentiment distribution for each drug
sentiment_summary = df_sentiment.groupBy("drugName", "sentiment") \
    .count() \
    .orderBy("drugName", "count", ascending=[True, False])

sentiment_summary.show(30, truncate=False)

# Grouping sentiment by condition
condition_sentiment = df_sentiment.groupBy("condition", "sentiment") \
    .count() \
    .orderBy("condition", "count", ascending=[True, False])

condition_sentiment.show(30, truncate=False)



sentiment_drug_pd = sentiment_summary.toPandas()

plt.figure(figsize=(12, 6))
sns.countplot(data=sentiment_drug_pd, x="sentiment", order=["positive", "neutral", "negative"])
plt.title("Overall Sentiment Distribution by Drug")
plt.xlabel("Sentiment")
plt.ylabel("Number of Reviews")
plt.show()

top_neg = sentiment_drug_pd[sentiment_drug_pd['sentiment'] == 'negative'] \
    .sort_values(by='count', ascending=False).head(10)

plt.figure(figsize=(12, 6))
sns.barplot(data=top_neg, x='count', y='drugName', palette='Reds_r')
plt.title("Top 10 Drugs with Most Negative Sentiment")
plt.xlabel("Count")
plt.ylabel("Drug Name")
plt.show()

"""3.Checking effectiveness for different medical conditions by clustering based for different medical conditions"""

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans

# Aggregate numeric features per condition
condition_summary = df_clean.groupBy("condition") \
    .agg(
        avg("rating").alias("avg_rating"),
        avg("usefulCount").alias("avg_useful")
    ).na.drop()

# Assemble features
vec_assembler = VectorAssembler(inputCols=["avg_rating", "avg_useful"], outputCol="features")
condition_vector = vec_assembler.transform(condition_summary)

kmeans = KMeans().setK(3).setSeed(1)
model = kmeans.fit(condition_vector)
clusters = model.transform(condition_vector)

clusters.select("condition", "avg_rating", "avg_useful", "prediction").show(20, truncate=False)

clusters_pd = clusters.select("condition", "avg_rating", "avg_useful", "prediction").toPandas()

plt.figure(figsize=(10, 6))
sns.scatterplot(data=clusters_pd, x='avg_rating', y='avg_useful', hue='prediction', palette='tab10', s=100)
plt.title("KMeans Clusters of Conditions by Avg Rating & Usefulness")
plt.xlabel("Average Rating")
plt.ylabel("Average Useful Count")
plt.legend(title="Cluster")
plt.show()

